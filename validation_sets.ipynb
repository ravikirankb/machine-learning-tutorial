{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Validation Sets "},{"metadata":{},"cell_type":"markdown","source":"###### The training sets does not provide an accurate signal of the quality of your model. In this section validation sets are used to improve the accuracy and provide model with additional layer of data for tuning the mode."},{"metadata":{},"cell_type":"markdown","source":"##### Dataset - we will use the california housing dataset to predict the median_house_value at the city block level.\n##### The california housing dataset has two separate datasets for training and test set\n* california_housing_train.csv\n* california_housing_test.csv"},{"metadata":{},"cell_type":"markdown","source":"##### Validation set is created by dividing the training set into two parts\n* Smaller training set\n* Validation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#@title Import modules\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\npd.options.display.max_rows = 10\npd.options.display.float_format = \"{:.1f}\".format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Load the California housing dataset\n\ntrain_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\ntest_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scale the label values to a factor of 1000.\nscale_factor = 1000.0\n\n# Scale the training set's label.\ntrain_df[\"median_house_value\"] /= scale_factor \n\n# Scale the test set's label\ntest_df[\"median_house_value\"] /= scale_factor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the functions to build and train the model.\n\n##### build_model -> defines the models topography\n##### train_model -> function to train the model based on the model topography defined in build_model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#@title Define the functions that build and train a model\ndef build_model(my_learning_rate):\n  ## Create and compile a simple linear regression model.\n  ## Most simple tf.keras models are sequential.\n  model = tf.keras.models.Sequential()\n\n  ## Add one linear layer to the model to yield a simple linear regressor.\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n\n  ## Compile the model topography into code that TensorFlow can efficiently\n  ## execute. Configure training to minimize the model's mean squared error. \n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n  return model  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, df, feature, label, my_epochs, \n                my_batch_size=None, my_validation_split=0.1):\n  ##Feed a dataset into the model in order to train it.\"\"\"\n\n  history = model.fit(x=df[feature],\n                      y=df[label],\n                      batch_size=my_batch_size,\n                      epochs=my_epochs,\n                      validation_split=my_validation_split)\n\n  ## Gather the model's trained weight and bias.\n  trained_weight = model.get_weights()[0]\n  trained_bias = model.get_weights()[1]\n\n  ## The list of epochs is stored separately from the \n  ## rest of history.\n  epochs = history.epoch\n  \n  ## Isolate the root mean squared error for each epoch.\n  hist = pd.DataFrame(history.history)\n  rmse = hist[\"root_mean_squared_error\"]\n\n  return epochs, rmse, history.history   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### plot the functions on the graph.\n\n#@title Define the plotting function\n\ndef plot_the_loss_curve(epochs, mae_training, mae_validation):\n  ##Plot a curve of loss vs. epoch.\"\"\"\n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Root Mean Squared Error\")\n\n  plt.plot(epochs[1:], mae_training[1:], label=\"Training Loss\")\n  plt.plot(epochs[1:], mae_validation[1:], label=\"Validation Loss\")\n  plt.legend()\n  \n  ## We're not going to plot the first epoch, since the loss on the first epoch\n  ## is often substantially greater than the loss for other epochs.\n  merged_mae_lists = mae_training[1:] + mae_validation[1:]\n  highest_loss = max(merged_mae_lists)\n  lowest_loss = min(merged_mae_lists)\n  delta = highest_loss - lowest_loss\n  print(delta)\n\n  top_of_y_axis = highest_loss + (delta * 0.05)\n  bottom_of_y_axis = lowest_loss - (delta * 0.05)\n   \n  plt.ylim([bottom_of_y_axis, top_of_y_axis])\n  plt.show()  \n\nprint(\"Defined the plot_the_loss_curve function.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We will experiment with the validation set\n\n###### We will set the validation_split variable at 0.2 which means 20% of the data will be used for validation set and 80% for training set.\n\n###### We will pass the variables to the build_model function and then use it to train the model and then plot the curves."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following variables are the hyperparameters.\nlearning_rate = 0.08\nepochs = 30\nbatch_size = 100\n\n# Split the original training set into a reduced training set and a\n# validation set. \nvalidation_split=0.2\n\n# Identify the feature and the label.\nmy_feature=\"median_income\"  # the median income on a specific city block.\nmy_label=\"median_house_value\" # the median value of a house on a specific city block.\n# That is, you're going to create a model that predicts house value based \n# solely on the neighborhood's median income.  \n\n# Discard any pre-existing version of the model.\nmy_model = None\n\n# Invoke the functions to build and train the model.\nmy_model = build_model(learning_rate)\nepochs, rmse, history = train_model(my_model, train_df, my_feature, \n                                    my_label, epochs, batch_size, \n                                    validation_split)\n\nplot_the_loss_curve(epochs, history[\"root_mean_squared_error\"], \n                    history[\"val_root_mean_squared_error\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> we can see from the graph that the trainingloss and the validation loss differs greatly, which means the data in the validation set is not similar to the data in the training set. This is a bit counter-intuitive as the purpose of validation set was to create an additional layer of data to improve model quality. But there is a reason for this error and also there is a fix.\nMost issues are with the data itself and so is the same in this particular case. We need to look into the data and identify what could potentially be wrong or different about the training set compared to the validation set.\n</b>"},{"metadata":{},"cell_type":"markdown","source":"#### Fix for the validation set data issue.\n\n<b> To fix this problem what one can do is to shuffle the data in the training_set before splitting into training_set and validation_set, this way we make sure the data points are spread across the entire dataset and there is some uniformity\nWe use the reindex method from the pandas library to reshuffle the input data.\nThen pass the shuffled input to the train_model function.\n</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#@title Double-click to view the complete implementation.\n\n# The following variables are the hyperparameters.\nlearning_rate = 0.08\nepochs = 70\nbatch_size = 100\n\n# Split the original training set into a reduced training set and a\n# validation set. \nvalidation_split=0.2\n\n# Identify the feature and the label.\nmy_feature=\"median_income\"  # the median income on a specific city block.\nmy_label=\"median_house_value\" # the median value of a house on a specific city block.\n# That is, you're going to create a model that predicts house value based \n# solely on the neighborhood's median income.  \n\n# Discard any pre-existing version of the model.\nmy_model = None\n\n# Shuffle the examples.\nshuffled_train_df = train_df.reindex(np.random.permutation(train_df.index)) \n\n# Invoke the functions to build and train the model. Train on the shuffled\n# training set.\nmy_model = build_model(learning_rate)\nepochs, rmse, history = train_model(my_model, shuffled_train_df, my_feature, \n                                    my_label, epochs, batch_size, \n                                    validation_split)\n\nplot_the_loss_curve(epochs, history[\"root_mean_squared_error\"], \n                    history[\"val_root_mean_squared_error\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Use the datasets to evaluate the models performance.\n\nx_test = test_df[my_feature]\ny_test = test_df[my_label]\n\nresults = my_model.evaluate(x_test, y_test, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}