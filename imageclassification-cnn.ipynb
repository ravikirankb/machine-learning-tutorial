{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Image classification convolution neural network"},{"metadata":{},"cell_type":"markdown","source":"<b> In this exercise we build an image classification model used to classify an image based on a class. A classic example is to determine whether an image is an apple or orange or in general of which fruit class the image belongs to.\nWe use tensorflow and keras to sequential model with convolution neural network with multi layers to achieve the desired goal.\n</b>"},{"metadata":{},"cell_type":"markdown","source":"<b> Lets first download the fruits dataset and then extract the data into test set and training set</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ['KAGGLE_USERNAME'] = \"uname\" # please use proper username \nos.environ['KAGGLE_KEY'] = \"kaggle_key\" # provide the kaggle key appropriately\ndownload_to_folder = '/tmp/fruits'\n!kaggle datasets download -d moltean/fruits -p download_to_folder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create the directories one each for training dataset and test datset\n\nbase_dir = '/tmp/fruits'\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\n\n# train directory with fruits pictures\ntrain_fruits_dir = os.path.join(train_dir, 'fruits')\n\n# validation directory with fruits pictures\nvalidation_fruits_dir = os.path.join(validation_dir, 'fruits')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom tqdm import tqdm\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## convolution neural network \n\n## we will feed images of specific sizes to our neural net for processing, we will use convolution + relu + maxpooling modules.\n\n# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n# the three color channels: R, G, and B\nimg_input = layers.Input(shape=(150, 150, 3))\nx = layers.Conv2D(16, 3, activation='relu')(img_input)\nx = layers.MaxPooling2D(2)(x)\nx = layers.Conv2D(32, 3, activation='relu')(x)\nx = layers.MaxPooling2D(2)(x)\nx = layers.Conv2D(64, 3, activation='relu')(x)\nx = layers.MaxPooling2D(2)(x)\n# Flatten feature map to a 1-dim tensor so we can add fully connected layers\nx = layers.Flatten()(x)\nx = layers.Dense(512, activation='relu')(x)\noutput = layers.Dense(1, activation='sigmoid')(x)\n\nmodel = Model(img_input, output)\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=RMSprop(lr=0.001),\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## data preprocessing\n## setting up data generators to read images into the source folder\n\n# scale image by 1./255\ntraining_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_datagen = ImageDataGenerator(rescale=1./255)\n\n# train iamges in batches of 25\ntrain_generator = training_datagen.flow_from_directory(\n        train_dir,  # source directory\n        target_size=(175, 175),\n        batch_size=25,\n        class_mode='categorical')\n\n# Flow validation images in batches of 20 using val_datagen generator\nvalidation_generator = val_datagen.flow_from_directory(\n        validation_dir,\n        target_size=(175, 175),\n        batch_size=25,\n        class_mode='categorial')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## train on all images\n\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=100, \n      epochs=25,\n      validation_data=validation_generator,\n      validation_steps=50,\n      verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### visualize the accuracy\n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}