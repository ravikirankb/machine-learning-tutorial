{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Feature Crosses\n##### In this exercise we learn about feature crosses and create code to use feature crosses and experiment with different ways to represent features\n\n##### We also learn to use different features of tensor flow and pands like\n * Use tf.feature_column methods to represent features in different ways.\n * Represent features as bins.\n * Cross bins to create a feature cross."},{"metadata":{},"cell_type":"markdown","source":"<b> We make use of the same californa housing dataset for our coding purpose </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%tensorflow_version 2.x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\n\nfrom matplotlib import pyplot as plt\n\n# The following lines adjust the granularity of reporting.\npd.options.display.max_rows = 10\npd.options.display.float_format = \"{:.1f}\".format\n\ntf.keras.backend.set_floatx('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> We do the same loading, scaling and shuffling of data before we use the dataset for training the model for better accuracy\nand also as a means to prepare the data.</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset\ntrain_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\ntest_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n\n# Scale the labels\nscale_factor = 1000.0\n# Scale the training set's label.\ntrain_df[\"median_house_value\"] /= scale_factor \n\n# Scale the test set's label\ntest_df[\"median_house_value\"] /= scale_factor\n\n# Shuffle the examples\ntrain_df = train_df.reindex(np.random.permutation(train_df.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> We will try to represent the Latitude and Longitude values as floating point values\n    To create feature colums we call</b> \n    * tf.feature_column to represent a single feature, single feature cross or a single synthetic feature. To represent a feature as a numeric floating point number we can use tf.feature_column.numeric_column, to represent as a bucket or bins use tf.feature_column.bucketized_column\n    * Add the columns into a python list\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a python list\nfeature_columns = []\n\n# numerical feature to represent latiutde\nlatitude = tf.feature_column.numeric_column(\"latitude\")\nfeature_columns.append(latitude)\n\n# numerical feature represent longitude.\nlongitude = tf.feature_column.numeric_column(\"longitude\")\n\n# add the features into the python list\nfeature_columns.append(longitude)\n\n# convert the features into a layer which will be part of the model.\nfp_feature_layer = layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets define the functions used to create the model\n\n  * create_model - defines the Tensorflow to build a linear regression model and use the fp_feature_layer to represent the model's feature\n  * train_model - trains the model which the specified features.\n  * plot_the_loss_curve - generates a loss curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define the functions\ndef create_model(my_learning_rate, feature_layer):\n  #Create and compile a simple linear regression model.\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(feature_layer)\n\n  # Add one linear layer to the model to yield a simple linear regressor.\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n\n  # Construct the layers into a model that TensorFlow can execute.\n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\n  return model           \n\n\ndef train_model(model, dataset, epochs, batch_size, label_name):\n  # we feed the dataset into the model here so it can train it.\n\n  features = {name:np.array(value) for name, value in dataset.items()}\n  label = np.array(features.pop(label_name))\n  history = model.fit(x=features, y=label, batch_size=batch_size,\n                      epochs=epochs, shuffle=True)\n\n  # store the list of epochs\n  epochs = history.epoch\n  \n  # Isolate the mean absolute error for each epoch.\n  hist = pd.DataFrame(history.history)\n  rmse = hist[\"root_mean_squared_error\"]\n\n  return epochs, rmse   \n\n\ndef plot_the_loss_curve(epochs, rmse):\n  # Plot the loss curve against the epoch.\n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Root Mean Squared Error\")\n\n  plt.plot(epochs, rmse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n  plt.show()  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We now train the model with the floating point representations that was created earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the hyperparameters.\nlearning_rate = 0.05\nepochs = 30\nbatch_size = 100\nlabel_name = 'median_house_value'\n\n# compile the model's topography.\nmy_model = create_model(learning_rate, fp_feature_layer)\n\n# Train the model.\nepochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n\nplot_the_loss_curve(epochs, rmse)\n\ntest_features = {name:np.array(value) for name, value in test_df.items()}\ntest_label = np.array(test_features.pop(label_name))\nmy_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Lets represent the Latitude and Longitude values as buckets.\n\n<b> We can create the latitude and longitude values as buckets or bins. Each bin represents all the neighbourhoods within a single degree. for example neighbourhoods within 34.4 to 34.8 are in a single bucket but neighbourhoods in 34.4 to 35.2 are in different buckets. \nThe model will learn a new weight for each bucket.\n    We create 10 buckets each for Latitude and Longitude </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"resolution_in_degrees = 1.0 \n\n# Create a python list to hold the features\nfeature_columns = []\n\n# Create a bucket feature for latitude.\nlatitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\nlatitude_boundaries = list(np.arange(int(min(train_df['latitude'])), \n                                     int(max(train_df['latitude'])), \n                                     resolution_in_degrees))\nlatitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, \n                                               latitude_boundaries)\nfeature_columns.append(latitude)\n\n# Create a bucket feature column for longitude.\nlongitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\nlongitude_boundaries = list(np.arange(int(min(train_df['longitude'])), \n                                      int(max(train_df['longitude'])), \n                                      resolution_in_degrees))\nlongitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n                                                longitude_boundaries)\nfeature_columns.append(longitude)\n\n# Convert the list of feature columns into a layer which is part of the model.\nbuckets_feature_layer = layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model with the bucketized representations\n\n# define the hyperparameters.\nlearning_rate = 0.04\nepochs = 35\n\n# Build the model passing in the buckets_feature_layer.\nmy_model = create_model(learning_rate, buckets_feature_layer)\n\n# Train the model.\nepochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n\nplot_the_loss_curve(epochs, rmse)\n\nmy_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### After you run the model we notice the bucket representation does better than the floating point representation of features.\n##### But we can do better by using feature crosses, ie, by using a feature cross of latitude and longitude and creating a single feature cross, as in real life scenarios the location exists in two dimensions of latitude and longitude it makes sense to use the latitude and longitude as a feature cross"},{"metadata":{"trusted":true},"cell_type":"code","source":"# following code demonstrates the feature cross.\n\nresolution_in_degrees = 1.0 \n\n# python list to hold generated feature column.\nfeature_columns = []\n\n# bucket feature column for latitude.\nlatitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\nlatitude_boundaries = list(np.arange(int(min(train_df['latitude'])), int(max(train_df['latitude'])), resolution_in_degrees))\nlatitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n\n# feature column for longitude.\nlongitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\nlongitude_boundaries = list(np.arange(int(min(train_df['longitude'])), int(max(train_df['longitude'])), resolution_in_degrees))\nlongitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, longitude_boundaries)\n\n# feature cross of latitude and longitude.\nlatitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=125)\ncrossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\nfeature_columns.append(crossed_feature)\n\n# Convert the list of feature columns into a layer to feed to the model.\nfeature_cross_feature_layer = layers.DenseFeatures(feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets run the model with the feature crossed columns.\n\n# define the hyperparameters.\nlearning_rate = 0.04\nepochs = 35\n\n# Build the model passing in the feature_cross_feature_layer: \nmy_model = create_model(learning_rate, feature_cross_feature_layer)\n\n# Train the model on the training set.\nepochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n\nplot_the_loss_curve(epochs, rmse)\n\nmy_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}