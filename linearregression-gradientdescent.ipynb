{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Linear Regression with Gradient Descent\n\nThe equation to update the model parameters(theta) is as follows: \n\n                         &theta;<sup>+</sup> = &theta;<sup>-</sup> + a/m(y<sub>i</sub> - h(x<sub>i</sub>)) $\\hat{a}$\n              \nThe cost function is given as  \n\n\n                         J(x,&theta;,y) = 1/2m (h(x<sub>i</sub>) - y<sub>i</sub>)<sup>2</sup>\n                        \n                         where h(x<sub>i</sub>) = &theta;<sup>T</sup>x"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Generate the random data, plot it as a graph\nslope = 10.23\nintercept = 2.34\ninput_var = np.arange(0.0,100.0)\noutput_var = slope * input_var + intercept + 500.0 * np.random.rand(len(input_var))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the graph with the randomly generated data.\nplt.figure()\nplt.scatter(input_var, output_var)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cost function to generate the cost for the given input\ndef cost_function(input_var,output_var,params):\n    no_samples = len(input_var)\n    cost_total = 0.0\n    for x,y in zip(input_var,output_var):\n        y_hat = np.dot(params, np.array([1.0, x]))\n        cost_total += (y_hat - y) ** 2\n    \n    cost = cost_total / (num_samples * 2.0)\n    \n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to perform the linear regression batch gradient descent\ndef linear_reg_batch_gradient(input_var,output_var,params,alpha,max_iterations):\n    iteration  = 0\n    no_samples = len(input_var)\n    cost = np.zeros(max_iterations)\n    params_cache = np.array([2,max_iterations])\n    \n    while iteration < max_iterations:\n        cost[iteration] = cost_function(input_var,output_var,params)\n         params_store[:, iteration] = params\n        \n        for x,y in zip(input_var, output_var):\n            y_hat = np.dot(params, np.array([1.0, x]))\n            gradient = np.array([1.0, x]) * (y - y_hat)\n            params += alpha * gradient/num_samples\n            \n        iteration += 1\n    \n    return params, cost, params_store","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(input_var, output_var, test_size=0.25)\n\nparams_0 = np.array([20.0,80.0])\n\nalpha = 1e-3\nmax_iterations = 500\nparams_hat_batch, cost_batch, params_store_batch =\\\n    linear_reg_batch_gradient(x_train, y_train, params_0, alpha, max_iterations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the params for linear regression using stochastic gradient descent\ndef lin_reg_stoch_gradient_descent(input_var, output_var, params, alpha):\n    num_samples = len(input_var)\n    cost = np.zeros(num_samples)\n    params_store = np.zeros([2, num_samples])\n    \n    i = 0\n    for x,y in zip(input_var, output_var):\n        cost[i] = compute_cost(input_var, output_var, params)\n        params_store[:, i] = params\n        \n        y_hat = np.dot(params, np.array([1.0, x]))\n        gradient = np.array([1.0, x]) * (y - y_hat)\n        params += alpha * gradient/num_samples\n        \n        i += 1\n            \n    return params, cost, params_store","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}