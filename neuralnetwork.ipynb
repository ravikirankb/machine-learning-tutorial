{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Deep neural network to build sophisticated linear regression model."},{"metadata":{},"cell_type":"markdown","source":"<b> Line in our previous examples we use the same California housing dataset and perform the same normalization on the data and perform the bucketing the dataset </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#@title Run on TensorFlow 2.x\n%tensorflow_version 2.x\nfrom __future__ import absolute_import, division, print_function, unicode_literals","execution_count":2,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"from __future__ imports must occur at the beginning of the file (<ipython-input-2-18e18b979c5f>, line 6)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-18e18b979c5f>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import relevant modules\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Adjust the granularity of reporting. \npd.options.display.max_rows = 10\npd.options.display.float_format = \"{:.1f}\".format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the dataset\ntrain_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\ntrain_df = train_df.reindex(np.random.permutation(train_df.index)) # shuffle the examples\ntest_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize the raw values to their z-scores.\n\n# Calculate the Z-scores of each column in the training set:\ntrain_df_mean = train_df.mean()\ntrain_df_std = train_df.std()\ntrain_df_norm = (train_df - train_df_mean)/train_df_std\n\n# Calculate the Z-scores of each column in the test set.\ntest_df_mean = test_df.mean()\ntest_df_std = test_df.std()\ntest_df_norm = (test_df - test_df_mean)/test_df_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Represent the data as three layers\n\n# empty python list to hold feature columns.\nfeature_columns = []\n\n# we use the resolution in Zs instead of degress, which is the standard deviation\nresolution_in_Zs = 0.3  # 3/10 of a standard deviation.\n\n# Bucket feature column for latitude.\nlatitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\nlatitude_boundaries = list(np.arange(int(min(train_df_norm['latitude'])), \n                                     int(max(train_df_norm['latitude'])), \n                                     resolution_in_Zs))\nlatitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n\n# Bucket feature column for longitude.\nlongitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\nlongitude_boundaries = list(np.arange(int(min(train_df_norm['longitude'])), \n                                      int(max(train_df_norm['longitude'])), \n                                      resolution_in_Zs))\nlongitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n                                                longitude_boundaries)\n\n# Feature cross of latitude and longitude.\nlatitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\ncrossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\nfeature_columns.append(crossed_feature)  \n\n# Represent median_income as a floating-point value.\nmedian_income = tf.feature_column.numeric_column(\"median_income\")\nfeature_columns.append(median_income)\n\n# Represent population as a floating-point value.\npopulation = tf.feature_column.numeric_column(\"population\")\nfeature_columns.append(population)\n\n# Convert the list of feature columns into a layer.\nmy_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)","execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tf' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7c18a44595dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Create a bucket feature column for latitude.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlatitude_as_a_numeric_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m latitude_boundaries = list(np.arange(int(min(train_df_norm['latitude'])), \n\u001b[1;32m     15\u001b[0m                                      \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Create the plotting function.\n\ndef plot_the_loss_curve(epochs, mse):\n  #Plot a curve of loss vs. epoch\n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Mean Squared Error\")\n\n  plt.plot(epochs, mse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### define the create and train model functions to perform the simple linear regression.\n\ndef create_model(my_learning_rate, feature_layer):\n  # Create and compile a simple linear regression model.\n  # Most simple tf.keras models are sequential.\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(feature_layer)\n\n  # Add one linear layer to the model to yield a simple linear regressor.\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n\n  # Construct the layers into a model that TensorFlow can execute.\n  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.MeanSquaredError()])\n\n  return model           \n\n\ndef train_model(model, dataset, epochs, batch_size, label_name):\n  # Split the dataset into features and label.\n  features = {name:np.array(value) for name, value in dataset.items()}\n  label = np.array(features.pop(label_name))\n  history = model.fit(x=features, y=label, batch_size=batch_size,\n                      epochs=epochs, shuffle=True)\n\n  # Get details that will be useful for plotting the loss curve.\n  epochs = history.epoch\n  hist = pd.DataFrame(history.history)\n  rmse = hist[\"mean_squared_error\"]\n\n  return epochs, rmse   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Run the following code and check for its output and verify the plot and the training and test losses curve.</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define hyperparameters.\nlearning_rate = 0.01\nepochs = 15\nbatch_size = 1000\nlabel_name = \"median_house_value\"\n\n# Establish the model's topography.\nmy_model = create_model(learning_rate, my_feature_layer)\n\n# Train the model on the normalized training set.\nepochs, mse = train_model(my_model, train_df_norm, epochs, batch_size, label_name)\nplot_the_loss_curve(epochs, mse)\n\ntest_features = {name:np.array(value) for name, value in test_df_norm.items()}\ntest_label = np.array(test_features.pop(label_name))\nprint(\"\\n Evaluate the linear regression model against the test set:\")\nmy_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Define a Neural net model\n    Create function create_model which defines the model's topogrpahy,\n    Defines\n    * No of layers in the neural network\n    * No of nodes in each layer\n    * Also define's the activation function of each layer. </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(my_learning_rate, my_feature_layer):\n  # Most simple tf.keras models are sequential.\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(my_feature_layer)\n\n  # units - specifies the no of nodes\n  # activation - specifies the activation function, 'Relu - rectified linear unit function'\n  # Define the first hidden layer with 20 nodes.   \n  model.add(tf.keras.layers.Dense(units=20, \n                                  activation='relu', \n                                  name='Hidden1'))\n  \n  # Define the second hidden layer with 12 nodes. \n  model.add(tf.keras.layers.Dense(units=12, \n                                  activation='relu', \n                                  name='Hidden2'))\n  \n  # Define the output layer.\n  model.add(tf.keras.layers.Dense(units=1,  \n                                  name='Output'))                              \n  \n  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.MeanSquaredError()])\n\n  return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Define the training function. </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, dataset, epochs, label_name,\n                batch_size=None):\n    \n  # Split the dataset into features and label.\n  features = {name:np.array(value) for name, value in dataset.items()}\n  label = np.array(features.pop(label_name))\n  history = model.fit(x=features, y=label, batch_size=batch_size,\n                      epochs=epochs, shuffle=True) \n\n  # The list of epochs is stored separately from the rest of history.\n  epochs = history.epoch\n  \n  # To track the progression of training we capture model's mean squared error.\n  hist = pd.DataFrame(history.history)\n  mse = hist[\"mean_squared_error\"]\n\n  return epochs, mse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### call the function to build and train deep neural network.\n\n# Define hyperparameters.\nlearning_rate = 0.01\nepochs = 20\nbatch_size = 1000\n\n# Specify label\nlabel_name = \"median_house_value\"\n\n# Model's topography.\nmy_model = create_model(learning_rate, my_feature_layer)\n\n# Train the model on the normalized training set.\nepochs, mse = train_model(my_model, train_df_norm, epochs, \n                          label_name, batch_size)\nplot_the_loss_curve(epochs, mse)\n\n# Test that model against the test set.\ntest_features = {name:np.array(value) for name, value in test_df_norm.items()}\ntest_label = np.array(test_features.pop(label_name)) # isolate the label\nprint(\"\\n Evaluate the new model against the test set:\")\nmy_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<B> Deep neural network optimization.\n    We can experiment with the number of layers in the deep neural network and the number of nodes in each layer, to achieve lower loss against test set and minimize the number of nodes in the deep neural net. </B>"},{"metadata":{},"cell_type":"markdown","source":"<B> We can also add regularization to the deep neural network for better optimization and to reduce overfitting of the model to the training set.\n    To implement regularization on a hidden layer specify the kernel_regularizer arguement to tf.keras.layers.Dense\n    * tf.keras.regularizers.l1 for L1 regularization\n    * tf.keras.regularizers.l2 for L2 regularization"},{"metadata":{},"cell_type":"markdown","source":"<b> In our model we can do the regularization by changing the model definition as below </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(tf.keras.layers.Dense(units=20, \n                                activation='relu',\n                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n                                name='Hidden1'))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}